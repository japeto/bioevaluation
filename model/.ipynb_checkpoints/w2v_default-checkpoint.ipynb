{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re  # For preprocessing\n",
    "import pandas as pd  # For data handling\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "import spacy  # For preprocessing\n",
    "import logging\n",
    "import glob\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file_name):\n",
    "    dataset = []\n",
    "    for line in open(file_name, \"r\"):\n",
    "        dataset.append( line.split(\" \") )\n",
    "    return dataset\n",
    "\n",
    "def flatten(vec):\n",
    "    return sum(vec, [])\n",
    "    \n",
    "def intersection(lst1, lst2): \n",
    "    lst1 = sum(lst1, [])\n",
    "    lst2 = sum(lst2, [])\n",
    "    lst3 = [{value} for value in lst1 if value in lst2] \n",
    "    return lst3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/macbookpro/Documents/BsC/S4/EvaluatingBioEmbeddings/data/bioinfer_sentences.txt', '/Users/macbookpro/Documents/BsC/S4/EvaluatingBioEmbeddings/data/semeval_sentences.txt', '/Users/macbookpro/Documents/BsC/S4/EvaluatingBioEmbeddings/data/ade_sentences.txt', '/Users/macbookpro/Documents/BsC/S4/EvaluatingBioEmbeddings/data/ace_sentences.txt']\n"
     ]
    }
   ],
   "source": [
    "files_words = glob.glob(\"/Users/macbookpro/Documents/BsC/S4/EvaluatingBioEmbeddings/data/*.txt\")\n",
    "print(files_words)\n",
    "vec_words = [load_file(file) for file in files_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"stimulation\" in flatten(vec_words[0])\n",
    "#\"stimulation\" in flatten(vec_words[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#intersection(vec_words[0], vec_words[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'these'},\n",
       " {'findings'},\n",
       " {'suggest'},\n",
       " {'that'},\n",
       " {'this'},\n",
       " {'novel'},\n",
       " {'is'},\n",
       " {'involved'},\n",
       " {'in'},\n",
       " {'the'},\n",
       " {'probably'},\n",
       " {'of'},\n",
       " {'activation'},\n",
       " {'and'},\n",
       " {'the'},\n",
       " {'stimulation'},\n",
       " {'of'},\n",
       " {'which'},\n",
       " {'leads'},\n",
       " {'to'},\n",
       " {'changes'},\n",
       " {'through'},\n",
       " {'the'},\n",
       " {'of'},\n",
       " {'which'},\n",
       " {'to'},\n",
       " {'binding'},\n",
       " {'such'},\n",
       " {'as'},\n",
       " {'and'},\n",
       " {'these'},\n",
       " {'findings'},\n",
       " {'suggest'},\n",
       " {'that'},\n",
       " {'this'},\n",
       " {'novel'},\n",
       " {'is'},\n",
       " {'involved'},\n",
       " {'in'},\n",
       " {'the'},\n",
       " {'probably'},\n",
       " {'of'},\n",
       " {'activation'},\n",
       " {'and'},\n",
       " {'the'},\n",
       " {'stimulation'},\n",
       " {'of'},\n",
       " {'which'},\n",
       " {'leads'},\n",
       " {'to'},\n",
       " {'changes'},\n",
       " {'through'},\n",
       " {'the'},\n",
       " {'of'},\n",
       " {'which'},\n",
       " {'to'},\n",
       " {'binding'},\n",
       " {'such'},\n",
       " {'as'},\n",
       " {'and'},\n",
       " {'these'},\n",
       " {'findings'},\n",
       " {'suggest'},\n",
       " {'that'},\n",
       " {'this'},\n",
       " {'novel'},\n",
       " {'is'},\n",
       " {'involved'},\n",
       " {'in'},\n",
       " {'the'},\n",
       " {'probably'},\n",
       " {'of'},\n",
       " {'activation'},\n",
       " {'and'},\n",
       " {'the'},\n",
       " {'stimulation'},\n",
       " {'of'},\n",
       " {'which'},\n",
       " {'leads'},\n",
       " {'to'},\n",
       " {'changes'},\n",
       " {'through'},\n",
       " {'the'},\n",
       " {'of'},\n",
       " {'which'},\n",
       " {'to'},\n",
       " {'binding'},\n",
       " {'such'},\n",
       " {'as'},\n",
       " {'and'},\n",
       " {'these'},\n",
       " {'findings'},\n",
       " {'suggest'},\n",
       " {'that'},\n",
       " {'this'},\n",
       " {'novel'},\n",
       " {'is'},\n",
       " {'involved'},\n",
       " {'in'},\n",
       " {'the'},\n",
       " {'probably'},\n",
       " {'of'},\n",
       " {'activation'},\n",
       " {'and'},\n",
       " {'the'},\n",
       " {'stimulation'},\n",
       " {'of'},\n",
       " {'which'},\n",
       " {'leads'},\n",
       " {'to'},\n",
       " {'changes'},\n",
       " {'through'},\n",
       " {'the'},\n",
       " {'of'},\n",
       " {'which'},\n",
       " {'to'},\n",
       " {'binding'},\n",
       " {'such'},\n",
       " {'as'},\n",
       " {'and'},\n",
       " {'these'},\n",
       " {'are'},\n",
       " {'similar'},\n",
       " {'to'},\n",
       " {'those'},\n",
       " {'of'},\n",
       " {'the'},\n",
       " {'of'},\n",
       " {'which'},\n",
       " {'an'},\n",
       " {'protein'},\n",
       " {'these'},\n",
       " {'are'},\n",
       " {'similar'},\n",
       " {'to'},\n",
       " {'those'},\n",
       " {'of'},\n",
       " {'the'},\n",
       " {'of'},\n",
       " {'which'},\n",
       " {'an'},\n",
       " {'protein'},\n",
       " {'these'},\n",
       " {'each'},\n",
       " {'have'},\n",
       " {'a'},\n",
       " {'v'},\n",
       " {'c'},\n",
       " {'and'},\n",
       " {'an'},\n",
       " {'a'},\n",
       " {'at'},\n",
       " {'the'},\n",
       " {'through'},\n",
       " {'which'},\n",
       " {'they'},\n",
       " {'the'},\n",
       " {'complex'},\n",
       " {'leading'},\n",
       " {'to'},\n",
       " {'rapid'},\n",
       " {'these'},\n",
       " {'each'},\n",
       " {'have'},\n",
       " {'a'},\n",
       " {'v'},\n",
       " {'c'},\n",
       " {'and'},\n",
       " {'an'},\n",
       " {'a'},\n",
       " {'at'},\n",
       " {'the'},\n",
       " {'through'},\n",
       " {'which'},\n",
       " {'they'},\n",
       " {'the'},\n",
       " {'complex'},\n",
       " {'leading'},\n",
       " {'to'},\n",
       " {'rapid'},\n",
       " {'the'},\n",
       " {'of'},\n",
       " {'the'},\n",
       " {'is'},\n",
       " {'similar'},\n",
       " {'to'},\n",
       " {'that'},\n",
       " {'of'},\n",
       " {'a'},\n",
       " {'of'},\n",
       " {'the'},\n",
       " {'protein'},\n",
       " {'the'},\n",
       " {'of'},\n",
       " {'the'},\n",
       " {'and'},\n",
       " {'are'},\n",
       " {'related'},\n",
       " {'to'},\n",
       " {'the'},\n",
       " {'cell'},\n",
       " {'and'},\n",
       " {'classical'},\n",
       " {'respectively'},\n",
       " {'the'},\n",
       " {'of'},\n",
       " {'the'},\n",
       " {'and'},\n",
       " {'are'},\n",
       " {'related'},\n",
       " {'to'},\n",
       " {'the'},\n",
       " {'cell'},\n",
       " {'and'},\n",
       " {'classical'},\n",
       " {'respectively'},\n",
       " {'these'},\n",
       " {'results'},\n",
       " {'that'},\n",
       " {'plays'},\n",
       " {'a'},\n",
       " {'role'},\n",
       " {'in'},\n",
       " {'in'},\n",
       " {'rapid'},\n",
       " {'of'},\n",
       " {'the'},\n",
       " {'cortical'},\n",
       " {'into'},\n",
       " {'these'},\n",
       " {'results'},\n",
       " {'the'},\n",
       " {'highly'},\n",
       " {'function'},\n",
       " {'of'},\n",
       " {'with'},\n",
       " {'low'},\n",
       " {'and'},\n",
       " {'their'},\n",
       " {'capacity'},\n",
       " {'with'},\n",
       " {'cell'},\n",
       " {'and'},\n",
       " {'proliferation'},\n",
       " {'these'},\n",
       " {'results'},\n",
       " {'indicate'},\n",
       " {'that'},\n",
       " {'i'},\n",
       " {'of'},\n",
       " {'myosin'},\n",
       " {'is'},\n",
       " {'ii'},\n",
       " {'containing'},\n",
       " {'and'},\n",
       " {'not'},\n",
       " {'may'},\n",
       " {'initial'},\n",
       " {'association'},\n",
       " {'of'},\n",
       " {'the'},\n",
       " {'with'},\n",
       " {'the'},\n",
       " {'and'},\n",
       " {'iii'},\n",
       " {'the'},\n",
       " {'site'},\n",
       " {'of'},\n",
       " {'where'},\n",
       " {'a'},\n",
       " {'significant'},\n",
       " {'of'},\n",
       " {'the'},\n",
       " {'will'},\n",
       " {'be'},\n",
       " {'into'},\n",
       " {'the'},\n",
       " {'these'},\n",
       " {'results'},\n",
       " {'indicate'},\n",
       " {'that'},\n",
       " {'i'},\n",
       " {'of'},\n",
       " {'myosin'},\n",
       " {'is'},\n",
       " {'ii'},\n",
       " {'containing'},\n",
       " {'and'},\n",
       " {'not'},\n",
       " {'may'},\n",
       " {'initial'},\n",
       " {'association'},\n",
       " {'of'},\n",
       " {'the'},\n",
       " {'with'},\n",
       " {'the'},\n",
       " {'and'},\n",
       " {'iii'},\n",
       " {'the'},\n",
       " {'site'},\n",
       " {'of'},\n",
       " {'where'},\n",
       " {'a'},\n",
       " {'significant'},\n",
       " {'of'},\n",
       " {'the'},\n",
       " {'will'},\n",
       " {'be'},\n",
       " {'into'},\n",
       " {'the'},\n",
       " {'these'},\n",
       " {'results'},\n",
       " {'indicate'},\n",
       " {'that'},\n",
       " {'of'},\n",
       " {'by'},\n",
       " {'and'},\n",
       " {'increased'},\n",
       " {'of'},\n",
       " {'by'},\n",
       " {'to'},\n",
       " {'of'},\n",
       " {'the'},\n",
       " {'these'},\n",
       " {'results'},\n",
       " {'indicate'},\n",
       " {'that'},\n",
       " {'of'},\n",
       " {'by'},\n",
       " {'and'},\n",
       " {'increased'},\n",
       " {'of'},\n",
       " {'by'},\n",
       " {'to'},\n",
       " {'of'},\n",
       " {'the'},\n",
       " {'these'},\n",
       " {'results'},\n",
       " {'indicate'},\n",
       " {'that'},\n",
       " {'of'},\n",
       " {'by'},\n",
       " {'and'},\n",
       " {'increased'},\n",
       " {'of'},\n",
       " {'by'},\n",
       " {'to'},\n",
       " {'of'},\n",
       " {'the'},\n",
       " {'these'},\n",
       " {'results'},\n",
       " {'indicate'},\n",
       " {'that'},\n",
       " {'the'},\n",
       " {'of'},\n",
       " {'and'},\n",
       " {'is'},\n",
       " {'by'},\n",
       " {'in'},\n",
       " {'the'},\n",
       " {'developing'},\n",
       " {'muscle'},\n",
       " {'and'},\n",
       " {'that'},\n",
       " {'the'},\n",
       " {'former'},\n",
       " {'may'},\n",
       " {'not'},\n",
       " {'be'},\n",
       " {'involved'},\n",
       " {'in'},\n",
       " {'these'},\n",
       " {'results'},\n",
       " {'show'},\n",
       " {'that'},\n",
       " {'is'},\n",
       " {'an'},\n",
       " {'important'},\n",
       " {'of'},\n",
       " {'cell'},\n",
       " {'during'},\n",
       " {'development'},\n",
       " {'these'},\n",
       " {'results'},\n",
       " {'show'},\n",
       " {'that'},\n",
       " {'is'},\n",
       " {'an'},\n",
       " {'important'},\n",
       " {'of'},\n",
       " {'cell'},\n",
       " {'during'},\n",
       " {'development'},\n",
       " {'these'},\n",
       " {'results'},\n",
       " {'show'},\n",
       " {'that'},\n",
       " {'the'},\n",
       " {'site'},\n",
       " {'is'},\n",
       " {'distinct'},\n",
       " {'from'},\n",
       " {'the'},\n",
       " {'recognized'},\n",
       " {'by'},\n",
       " {'the'},\n",
       " {'antibody'},\n",
       " {'the'},\n",
       " {'possibility'},\n",
       " {'that'},\n",
       " {'it'},\n",
       " {'to'},\n",
       " {'a'},\n",
       " {'novel'},\n",
       " {'important'},\n",
       " {'site'},\n",
       " {'in'},\n",
       " {'the'},\n",
       " {'these'},\n",
       " {'results'},\n",
       " {'suggest'},\n",
       " {'that'},\n",
       " {'activation'},\n",
       " {'of'},\n",
       " {'the'},\n",
       " {'activation'},\n",
       " {'of'},\n",
       " {'and'},\n",
       " {'and'},\n",
       " {'that'},\n",
       " {'under'},\n",
       " {'certain'},\n",
       " {'circumstances'},\n",
       " {'activation'},\n",
       " {'of'},\n",
       " {'the'},\n",
       " {'and'},\n",
       " {'the'},\n",
       " {'increased'},\n",
       " {'of'},\n",
       " {'and'},\n",
       " {'may'},\n",
       " {'be'},\n",
       " {'involved'},\n",
       " {'in'},\n",
       " {'the'},\n",
       " {'of'},\n",
       " {'in'},\n",
       " {'cells'},\n",
       " {'possibly'},\n",
       " {'through'},\n",
       " {'the'},\n",
       " {'interaction'},\n",
       " {'of'},\n",
       " {'the'},\n",
       " {'with'},\n",
       " {'such'},\n",
       " {'as'},\n",
       " {'and'},\n",
       " {'these'},\n",
       " {'results'},\n",
       " {'suggest'},\n",
       " {'that'},\n",
       " {'activation'},\n",
       " {'of'},\n",
       " {'the'},\n",
       " {'activation'},\n",
       " {'of'},\n",
       " {'and'},\n",
       " {'and'},\n",
       " {'that'},\n",
       " {'under'},\n",
       " {'certain'},\n",
       " {'circumstances'},\n",
       " {'activation'},\n",
       " {'of'},\n",
       " {'the'},\n",
       " {'and'},\n",
       " {'the'},\n",
       " {'increased'},\n",
       " {'of'},\n",
       " {'and'},\n",
       " {'may'},\n",
       " {'be'},\n",
       " {'involved'},\n",
       " {'in'},\n",
       " {'the'},\n",
       " {'of'},\n",
       " {'in'},\n",
       " {'cells'},\n",
       " {'possibly'},\n",
       " {'through'},\n",
       " {'the'},\n",
       " {'interaction'},\n",
       " {'of'},\n",
       " {'the'},\n",
       " {'with'},\n",
       " {'such'},\n",
       " {'as'},\n",
       " {'and'},\n",
       " {'these'},\n",
       " {'results'},\n",
       " {'suggest'},\n",
       " {'that'},\n",
       " {'activation'},\n",
       " {'of'},\n",
       " {'the'},\n",
       " {'activation'},\n",
       " {'of'},\n",
       " {'and'},\n",
       " {'and'},\n",
       " {'that'},\n",
       " {'under'},\n",
       " {'certain'},\n",
       " {'circumstances'},\n",
       " {'activation'},\n",
       " {'of'},\n",
       " {'the'},\n",
       " {'and'},\n",
       " {'the'},\n",
       " {'increased'},\n",
       " {'of'},\n",
       " {'and'},\n",
       " {'may'},\n",
       " {'be'},\n",
       " {'involved'},\n",
       " {'in'},\n",
       " {'the'},\n",
       " {'of'},\n",
       " {'in'},\n",
       " {'cells'},\n",
       " {'possibly'},\n",
       " {'through'},\n",
       " {'the'},\n",
       " {'interaction'},\n",
       " {'of'},\n",
       " {'the'},\n",
       " {'with'},\n",
       " {'such'},\n",
       " {'as'},\n",
       " {'and'},\n",
       " {'these'},\n",
       " {'results'},\n",
       " {'suggest'},\n",
       " {'that'},\n",
       " {'activation'},\n",
       " {'of'},\n",
       " {'the'},\n",
       " {'activation'},\n",
       " {'of'},\n",
       " {'and'},\n",
       " {'and'},\n",
       " {'that'},\n",
       " {'under'},\n",
       " {'certain'},\n",
       " {'circumstances'},\n",
       " {'activation'},\n",
       " {'of'},\n",
       " {'the'},\n",
       " {'and'},\n",
       " {'the'},\n",
       " {'increased'},\n",
       " {'of'},\n",
       " {'and'},\n",
       " {'may'},\n",
       " {'be'},\n",
       " {'involved'},\n",
       " {'in'},\n",
       " {'the'},\n",
       " {'of'},\n",
       " {'in'},\n",
       " {'cells'},\n",
       " {'possibly'},\n",
       " {'through'},\n",
       " {'the'},\n",
       " {'interaction'},\n",
       " {'of'},\n",
       " {'the'},\n",
       " {'with'},\n",
       " {'such'},\n",
       " {'as'},\n",
       " {'and'},\n",
       " {'these'},\n",
       " {'results'},\n",
       " {'suggest'},\n",
       " {'that'},\n",
       " {'may'},\n",
       " {'be'},\n",
       " {'involved'},\n",
       " {'in'},\n",
       " {'of'},\n",
       " {'membranous'},\n",
       " {'these'},\n",
       " {'results'},\n",
       " {'suggest'},\n",
       " {'that'},\n",
       " {'an'},\n",
       " {'as'},\n",
       " {'link'},\n",
       " {'between'},\n",
       " {'cell'},\n",
       " {'surface'},\n",
       " {'and'},\n",
       " {'of'},\n",
       " {'the'},\n",
       " {'these'},\n",
       " {'results'},\n",
       " {'suggest'},\n",
       " {'that'},\n",
       " {'may'},\n",
       " {'be'},\n",
       " {'involved'},\n",
       " {'in'},\n",
       " {'the'},\n",
       " {'pathogenesis'},\n",
       " {'of'},\n",
       " {'glomerulonephritis'},\n",
       " {'by'},\n",
       " {'these'},\n",
       " {'results'},\n",
       " {'suggest'},\n",
       " {'that'},\n",
       " {'may'},\n",
       " {'the'},\n",
       " {'of'},\n",
       " {'protein'},\n",
       " {'such'},\n",
       " {'as'},\n",
       " {'and'},\n",
       " {'through'},\n",
       " {'a'},\n",
       " {'direct'},\n",
       " {'physical'},\n",
       " {'interaction'},\n",
       " {'these'},\n",
       " {'results'},\n",
       " {'suggest'},\n",
       " {'that'},\n",
       " {'of'},\n",
       " {'the'},\n",
       " {'and'},\n",
       " {'plays'},\n",
       " {'a'},\n",
       " {'role'},\n",
       " {'in'},\n",
       " {'of'},\n",
       " {'filaments'},\n",
       " {'and'},\n",
       " {'by'},\n",
       " {'these'},\n",
       " {'results'},\n",
       " {'suggest'},\n",
       " {'that'},\n",
       " {'of'},\n",
       " {'the'},\n",
       " {'and'},\n",
       " {'plays'},\n",
       " {'a'},\n",
       " {'role'},\n",
       " {'in'},\n",
       " {'of'},\n",
       " {'filaments'},\n",
       " {'and'},\n",
       " {'by'},\n",
       " {'these'},\n",
       " {'results'},\n",
       " {'suggest'},\n",
       " {'that'},\n",
       " {'the'},\n",
       " {'dysfunction'},\n",
       " {'of'},\n",
       " {'in'},\n",
       " {'these'},\n",
       " {'cell'},\n",
       " {'is'},\n",
       " {'due'},\n",
       " {'to'},\n",
       " {'its'},\n",
       " {'failure'},\n",
       " {'to'},\n",
       " {'interact'},\n",
       " {'with'},\n",
       " {'and'},\n",
       " {'that'},\n",
       " {'this'},\n",
       " {'defect'},\n",
       " {'results'},\n",
       " {'from'},\n",
       " {'the'},\n",
       " {'mutation'},\n",
       " {'in'},\n",
       " {'these'},\n",
       " {'results'},\n",
       " {'suggest'},\n",
       " {'that'},\n",
       " {'the'},\n",
       " {'dysfunction'},\n",
       " {'of'},\n",
       " {'in'},\n",
       " {'these'},\n",
       " {'cell'},\n",
       " {'is'},\n",
       " {'due'},\n",
       " {'to'},\n",
       " {'its'},\n",
       " {'failure'},\n",
       " {'to'},\n",
       " {'interact'},\n",
       " {'with'},\n",
       " {'and'},\n",
       " {'that'},\n",
       " {'this'},\n",
       " {'defect'},\n",
       " {'results'},\n",
       " {'from'},\n",
       " {'the'},\n",
       " {'mutation'},\n",
       " {'in'},\n",
       " {'these'},\n",
       " {'results'},\n",
       " {'suggest'},\n",
       " {'that'},\n",
       " {'the'},\n",
       " {'distribution'},\n",
       " {'of'},\n",
       " {'as'},\n",
       " {'well'},\n",
       " {'as'},\n",
       " {'its'},\n",
       " {'interaction'},\n",
       " {'with'},\n",
       " {'in'},\n",
       " {'is'},\n",
       " {'by'},\n",
       " {'its'},\n",
       " {'and'},\n",
       " {'these'},\n",
       " {'results'},\n",
       " {'suggest'},\n",
       " {'that'},\n",
       " {'the'},\n",
       " {'distribution'},\n",
       " {'of'},\n",
       " {'as'},\n",
       " {'well'},\n",
       " {'as'},\n",
       " {'its'},\n",
       " {'interaction'},\n",
       " {'with'},\n",
       " {'in'},\n",
       " {'is'},\n",
       " {'by'},\n",
       " {'its'},\n",
       " {'and'},\n",
       " {'these'},\n",
       " {'results'},\n",
       " {'suggest'},\n",
       " {'that'},\n",
       " {'can'},\n",
       " {'cause'},\n",
       " {'in'},\n",
       " {'pancreatic'},\n",
       " {'beta'},\n",
       " {'cells'},\n",
       " {'through'},\n",
       " {'factors'},\n",
       " {'and'},\n",
       " {'and'},\n",
       " {'production'},\n",
       " {'may'},\n",
       " {'be'},\n",
       " {'involved'},\n",
       " {'in'},\n",
       " {'the'},\n",
       " {'these'},\n",
       " {'results'},\n",
       " {'suggest'},\n",
       " {'that'},\n",
       " {'can'},\n",
       " {'cause'},\n",
       " {'in'},\n",
       " {'pancreatic'},\n",
       " {'beta'},\n",
       " {'cells'},\n",
       " {'through'},\n",
       " {'factors'},\n",
       " {'and'},\n",
       " {'and'},\n",
       " {'production'},\n",
       " {'may'},\n",
       " {'be'},\n",
       " {'involved'},\n",
       " {'in'},\n",
       " {'the'},\n",
       " {'these'},\n",
       " {'results'},\n",
       " {'support'},\n",
       " {'a'},\n",
       " {'in'},\n",
       " {'which'},\n",
       " {'stimulation'},\n",
       " {'to'},\n",
       " {'the'},\n",
       " {'leading'},\n",
       " {'where'},\n",
       " {'its'},\n",
       " {'activity'},\n",
       " {'is'},\n",
       " {'activated'},\n",
       " {'leading'},\n",
       " {'to'},\n",
       " {'the'},\n",
       " {'generation'},\n",
       " {'of'},\n",
       " {'short'},\n",
       " {'filaments'},\n",
       " {'with'},\n",
       " {'free'},\n",
       " {'that'},\n",
       " {'in'},\n",
       " {'the'},\n",
       " {'of'},\n",
       " {'these'},\n",
       " {'results'},\n",
       " {'support'},\n",
       " {'a'},\n",
       " {'in'},\n",
       " {'which'},\n",
       " {'stimulation'},\n",
       " {'to'},\n",
       " {'the'},\n",
       " {'leading'},\n",
       " {'where'},\n",
       " {'its'},\n",
       " {'activity'},\n",
       " {'is'},\n",
       " {'activated'},\n",
       " {'leading'},\n",
       " {'to'},\n",
       " {'the'},\n",
       " {'generation'},\n",
       " {'of'},\n",
       " {'short'},\n",
       " {'filaments'},\n",
       " {'with'},\n",
       " {'free'},\n",
       " {'that'},\n",
       " {'in'},\n",
       " {'the'},\n",
       " {'of'},\n",
       " {'these'},\n",
       " {'results'},\n",
       " {'support'},\n",
       " {'a'},\n",
       " {'in'},\n",
       " {'which'},\n",
       " {'stimulation'},\n",
       " {'to'},\n",
       " {'the'},\n",
       " {'leading'},\n",
       " {'where'},\n",
       " {'its'},\n",
       " {'activity'},\n",
       " {'is'},\n",
       " {'activated'},\n",
       " {'leading'},\n",
       " {'to'},\n",
       " {'the'},\n",
       " {'generation'},\n",
       " {'of'},\n",
       " {'short'},\n",
       " {'filaments'},\n",
       " {'with'},\n",
       " {'free'},\n",
       " {'that'},\n",
       " {'in'},\n",
       " {'the'},\n",
       " {'of'},\n",
       " {'these'},\n",
       " {'results'},\n",
       " {'support'},\n",
       " {'a'},\n",
       " {'in'},\n",
       " {'which'},\n",
       " {'stimulation'},\n",
       " {'to'},\n",
       " {'the'},\n",
       " {'leading'},\n",
       " {'where'},\n",
       " {'its'},\n",
       " {'activity'},\n",
       " {'is'},\n",
       " {'activated'},\n",
       " {'leading'},\n",
       " {'to'},\n",
       " {'the'},\n",
       " {'generation'},\n",
       " {'of'},\n",
       " {'short'},\n",
       " {'filaments'},\n",
       " {'with'},\n",
       " {'free'},\n",
       " {'that'},\n",
       " {'in'},\n",
       " {'the'},\n",
       " {'of'},\n",
       " {'the'},\n",
       " {'and'},\n",
       " {'with'},\n",
       " {'and'},\n",
       " {'the'},\n",
       " {'protein'},\n",
       " {'in'},\n",
       " {'human'},\n",
       " {'lymphocytes'},\n",
       " {'the'},\n",
       " {'and'},\n",
       " {'with'},\n",
       " {'and'},\n",
       " {'the'},\n",
       " {'protein'},\n",
       " {'in'},\n",
       " {'human'},\n",
       " {'lymphocytes'},\n",
       " {'these'},\n",
       " {'two'},\n",
       " {'were'},\n",
       " {'identified'},\n",
       " {'as'},\n",
       " {'the'},\n",
       " {'and'},\n",
       " {'of'},\n",
       " {'the'},\n",
       " {'protein'},\n",
       " {'by'},\n",
       " {'of'},\n",
       " {'and'},\n",
       " {'with'},\n",
       " {'a'},\n",
       " {'monoclonal'},\n",
       " {'antibody'},\n",
       " {'specific'},\n",
       " {'for'},\n",
       " {'the'},\n",
       " {'3'},\n",
       " {'of'},\n",
       " {'a'},\n",
       " {'type'},\n",
       " {'i'},\n",
       " {'myosin'},\n",
       " {'to'},\n",
       " {'and'},\n",
       " {'is'},\n",
       " {'required'},\n",
       " {'for'},\n",
       " {'to'},\n",
       " {'sites'},\n",
       " {'of'},\n",
       " {'the'},\n",
       " {'3'},\n",
       " {'of'},\n",
       " {'a'},\n",
       " {'type'},\n",
       " {'i'},\n",
       " {'myosin'},\n",
       " {'to'},\n",
       " {'and'},\n",
       " {'is'},\n",
       " {'required'},\n",
       " {'for'},\n",
       " {'to'},\n",
       " {'sites'},\n",
       " {'of'},\n",
       " {'the'},\n",
       " {'structure'},\n",
       " {'of'},\n",
       " {'an'},\n",
       " {'from'},\n",
       " {'human'},\n",
       " {'the'},\n",
       " {'structure'},\n",
       " {'of'},\n",
       " {'entirely'},\n",
       " {'from'},\n",
       " {'that'},\n",
       " {'of'},\n",
       " {'and'},\n",
       " {'the'},\n",
       " {'of'},\n",
       " {'the'},\n",
       " {'protein'},\n",
       " {'the'},\n",
       " {'structure'},\n",
       " {'of'},\n",
       " {'entirely'},\n",
       " {'from'},\n",
       " {'that'},\n",
       " {'of'},\n",
       " {'and'},\n",
       " {'the'},\n",
       " ...]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [Word2Vec(sentences=sents, size=300) for sents in vec_words]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-23 20:21:51,155 : INFO : collecting all words and their counts\n",
      "2019-05-23 20:21:51,163 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-05-23 20:21:51,181 : INFO : collected 3412 word types from a corpus of 29275 raw words and 964 sentences\n",
      "2019-05-23 20:21:51,183 : INFO : Loading a fresh vocabulary\n",
      "2019-05-23 20:21:51,194 : INFO : effective_min_count=5 retains 858 unique words (25% of original 3412, drops 2554)\n",
      "2019-05-23 20:21:51,196 : INFO : effective_min_count=5 leaves 24447 word corpus (83% of original 29275, drops 4828)\n",
      "2019-05-23 20:21:51,208 : INFO : deleting the raw counts dictionary of 3412 items\n",
      "2019-05-23 20:21:51,210 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2019-05-23 20:21:51,212 : INFO : downsampling leaves estimated 15606 word corpus (63.8% of prior 24447)\n",
      "2019-05-23 20:21:51,221 : INFO : estimated required memory for 858 words and 300 dimensions: 2488200 bytes\n",
      "2019-05-23 20:21:51,224 : INFO : resetting layer weights\n",
      "2019-05-23 20:21:51,262 : INFO : training model with 3 workers on 858 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-23 20:21:51,306 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-23 20:21:51,315 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-23 20:21:51,319 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-23 20:21:51,321 : INFO : EPOCH - 1 : training on 29275 raw words (15593 effective words) took 0.0s, 337743 effective words/s\n",
      "2019-05-23 20:21:51,357 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-23 20:21:51,365 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-23 20:21:51,367 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-23 20:21:51,369 : INFO : EPOCH - 2 : training on 29275 raw words (15514 effective words) took 0.0s, 382572 effective words/s\n",
      "2019-05-23 20:21:51,416 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-23 20:21:51,424 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-23 20:21:51,426 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-23 20:21:51,427 : INFO : EPOCH - 3 : training on 29275 raw words (15562 effective words) took 0.0s, 312784 effective words/s\n",
      "2019-05-23 20:21:51,468 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-23 20:21:51,480 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-23 20:21:51,485 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-23 20:21:51,486 : INFO : EPOCH - 4 : training on 29275 raw words (15696 effective words) took 0.0s, 359407 effective words/s\n",
      "2019-05-23 20:21:51,524 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-23 20:21:51,535 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-23 20:21:51,537 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-23 20:21:51,539 : INFO : EPOCH - 5 : training on 29275 raw words (15586 effective words) took 0.0s, 375678 effective words/s\n",
      "2019-05-23 20:21:51,541 : INFO : training on a 146375 raw words (77951 effective words) took 0.3s, 280471 effective words/s\n",
      "2019-05-23 20:21:51,544 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-23 20:21:51,547 : INFO : collecting all words and their counts\n",
      "2019-05-23 20:21:51,549 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-05-23 20:21:51,630 : INFO : collected 18507 word types from a corpus of 169088 raw words and 7068 sentences\n",
      "2019-05-23 20:21:51,632 : INFO : Loading a fresh vocabulary\n",
      "2019-05-23 20:21:51,675 : INFO : effective_min_count=5 retains 4319 unique words (23% of original 18507, drops 14188)\n",
      "2019-05-23 20:21:51,676 : INFO : effective_min_count=5 leaves 145953 word corpus (86% of original 169088, drops 23135)\n",
      "2019-05-23 20:21:51,720 : INFO : deleting the raw counts dictionary of 18507 items\n",
      "2019-05-23 20:21:51,722 : INFO : sample=0.001 downsamples 37 most-common words\n",
      "2019-05-23 20:21:51,724 : INFO : downsampling leaves estimated 103963 word corpus (71.2% of prior 145953)\n",
      "2019-05-23 20:21:51,753 : INFO : estimated required memory for 4319 words and 300 dimensions: 12525100 bytes\n",
      "2019-05-23 20:21:51,755 : INFO : resetting layer weights\n",
      "2019-05-23 20:21:51,962 : INFO : training model with 3 workers on 4319 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-23 20:21:52,317 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-23 20:21:52,323 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-23 20:21:52,333 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-23 20:21:52,335 : INFO : EPOCH - 1 : training on 169088 raw words (103973 effective words) took 0.4s, 293547 effective words/s\n",
      "2019-05-23 20:21:52,601 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-23 20:21:52,613 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-23 20:21:52,621 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-23 20:21:52,623 : INFO : EPOCH - 2 : training on 169088 raw words (103715 effective words) took 0.3s, 396303 effective words/s\n",
      "2019-05-23 20:21:52,986 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-23 20:21:53,020 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-23 20:21:53,036 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-23 20:21:53,038 : INFO : EPOCH - 3 : training on 169088 raw words (103887 effective words) took 0.4s, 259375 effective words/s\n",
      "2019-05-23 20:21:53,291 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-23 20:21:53,316 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-23 20:21:53,317 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-23 20:21:53,319 : INFO : EPOCH - 4 : training on 169088 raw words (104058 effective words) took 0.3s, 392650 effective words/s\n",
      "2019-05-23 20:21:53,561 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-23 20:21:53,577 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-23 20:21:53,584 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-23 20:21:53,585 : INFO : EPOCH - 5 : training on 169088 raw words (103865 effective words) took 0.2s, 415698 effective words/s\n",
      "2019-05-23 20:21:53,588 : INFO : training on a 845440 raw words (519498 effective words) took 1.6s, 320045 effective words/s\n",
      "2019-05-23 20:21:53,591 : INFO : collecting all words and their counts\n",
      "2019-05-23 20:21:53,594 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-05-23 20:21:53,619 : INFO : collected 6499 word types from a corpus of 47388 raw words and 2471 sentences\n",
      "2019-05-23 20:21:53,620 : INFO : Loading a fresh vocabulary\n",
      "2019-05-23 20:21:53,639 : INFO : effective_min_count=5 retains 1435 unique words (22% of original 6499, drops 5064)\n",
      "2019-05-23 20:21:53,640 : INFO : effective_min_count=5 leaves 38790 word corpus (81% of original 47388, drops 8598)\n",
      "2019-05-23 20:21:53,654 : INFO : deleting the raw counts dictionary of 6499 items\n",
      "2019-05-23 20:21:53,656 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2019-05-23 20:21:53,660 : INFO : downsampling leaves estimated 27171 word corpus (70.0% of prior 38790)\n",
      "2019-05-23 20:21:53,670 : INFO : estimated required memory for 1435 words and 300 dimensions: 4161500 bytes\n",
      "2019-05-23 20:21:53,671 : INFO : resetting layer weights\n",
      "2019-05-23 20:21:53,732 : INFO : training model with 3 workers on 1435 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-23 20:21:53,797 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-23 20:21:53,815 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-23 20:21:53,818 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-23 20:21:53,819 : INFO : EPOCH - 1 : training on 47388 raw words (27124 effective words) took 0.1s, 402752 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-23 20:21:53,881 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-23 20:21:53,901 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-23 20:21:53,908 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-23 20:21:53,910 : INFO : EPOCH - 2 : training on 47388 raw words (27220 effective words) took 0.1s, 365857 effective words/s\n",
      "2019-05-23 20:21:53,982 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-23 20:21:54,004 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-23 20:21:54,009 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-23 20:21:54,011 : INFO : EPOCH - 3 : training on 47388 raw words (27293 effective words) took 0.1s, 393315 effective words/s\n",
      "2019-05-23 20:21:54,078 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-23 20:21:54,102 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-23 20:21:54,105 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-23 20:21:54,107 : INFO : EPOCH - 4 : training on 47388 raw words (27161 effective words) took 0.1s, 368827 effective words/s\n",
      "2019-05-23 20:21:54,179 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-23 20:21:54,199 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-23 20:21:54,201 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-23 20:21:54,202 : INFO : EPOCH - 5 : training on 47388 raw words (27241 effective words) took 0.1s, 345684 effective words/s\n",
      "2019-05-23 20:21:54,204 : INFO : training on a 236940 raw words (136039 effective words) took 0.5s, 289213 effective words/s\n",
      "2019-05-23 20:21:54,206 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-05-23 20:21:54,207 : INFO : collecting all words and their counts\n",
      "2019-05-23 20:21:54,209 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-05-23 20:21:54,309 : INFO : collected 13325 word types from a corpus of 225651 raw words and 7972 sentences\n",
      "2019-05-23 20:21:54,313 : INFO : Loading a fresh vocabulary\n",
      "2019-05-23 20:21:54,346 : INFO : effective_min_count=5 retains 5159 unique words (38% of original 13325, drops 8166)\n",
      "2019-05-23 20:21:54,348 : INFO : effective_min_count=5 leaves 209092 word corpus (92% of original 225651, drops 16559)\n",
      "2019-05-23 20:21:54,396 : INFO : deleting the raw counts dictionary of 13325 items\n",
      "2019-05-23 20:21:54,398 : INFO : sample=0.001 downsamples 37 most-common words\n",
      "2019-05-23 20:21:54,399 : INFO : downsampling leaves estimated 153153 word corpus (73.2% of prior 209092)\n",
      "2019-05-23 20:21:54,440 : INFO : estimated required memory for 5159 words and 300 dimensions: 14961100 bytes\n",
      "2019-05-23 20:21:54,444 : INFO : resetting layer weights\n",
      "2019-05-23 20:21:54,618 : INFO : training model with 3 workers on 5159 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-23 20:21:55,218 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-23 20:21:55,235 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-23 20:21:55,257 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-23 20:21:55,258 : INFO : EPOCH - 1 : training on 225651 raw words (153134 effective words) took 0.6s, 244877 effective words/s\n",
      "2019-05-23 20:21:55,647 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-23 20:21:55,665 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-23 20:21:55,682 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-23 20:21:55,683 : INFO : EPOCH - 2 : training on 225651 raw words (152926 effective words) took 0.4s, 369375 effective words/s\n",
      "2019-05-23 20:21:56,009 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-23 20:21:56,028 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-23 20:21:56,030 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-23 20:21:56,033 : INFO : EPOCH - 3 : training on 225651 raw words (153152 effective words) took 0.3s, 460695 effective words/s\n",
      "2019-05-23 20:21:56,654 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-23 20:21:56,679 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-23 20:21:56,708 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-23 20:21:56,710 : INFO : EPOCH - 4 : training on 225651 raw words (153229 effective words) took 0.7s, 230871 effective words/s\n",
      "2019-05-23 20:21:57,087 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-23 20:21:57,112 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-23 20:21:57,138 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-23 20:21:57,139 : INFO : EPOCH - 5 : training on 225651 raw words (153059 effective words) took 0.4s, 376442 effective words/s\n",
      "2019-05-23 20:21:57,141 : INFO : training on a 1128255 raw words (765500 effective words) took 2.5s, 303606 effective words/s\n"
     ]
    }
   ],
   "source": [
    "models[0].wv.most_similar(\"protein\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-23 20:22:02,688 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('to', 0.9999743700027466),\n",
       " ('proteins', 0.9999728798866272),\n",
       " ('of', 0.9999727010726929),\n",
       " ('by', 0.999972403049469),\n",
       " ('with', 0.9999718070030212),\n",
       " ('a', 0.9999715685844421),\n",
       " ('.\\n', 0.9999715089797974),\n",
       " ('or', 0.9999710321426392),\n",
       " ('for', 0.9999707937240601),\n",
       " ('is', 0.9999704360961914)]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#models[0].wv.most_similar(positive=[\"stimulation\", \"protein\"], negative=[\"binding\"])\n",
    "models[0].wv.most_similar(positive=[\"protein\"], negative=[\"protein\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('helped', 0.999786376953125),\n",
       " ('Dick', 0.9997806549072266),\n",
       " ('David', 0.9997518658638),\n",
       " ('itself', 0.9997458457946777),\n",
       " ('Richard', 0.9997393488883972),\n",
       " ('ring', 0.9997374415397644),\n",
       " ('effort', 0.9997148513793945),\n",
       " ('Security', 0.9997009038925171),\n",
       " ('race', 0.9996994733810425),\n",
       " ('ministry', 0.9996874928474426)]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"presidential\" in flatten(vec_words[3])\n",
    "models[3].wv.most_similar(\"presidential\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
